{
    "data": [
        {
            "topicTitle": "Access local and kubectl-based Logs",
            "topicIds": ["ctx-management-clusters"],
            "htmlContent": "\n\n<p>Tanzu Community Edition retains logs for management and workload cluster deployment and operation.</p>\n\n<h4>Using kubectl-based Cluster Logs</h4>\n\n<p>If local cluster logs do not provide sufficient information, you can retrieve logs using kubectl as follows:</p>\n\n<h5>Using kubectl-based Cluster Logs</h5>\n\n<p>\n    To monitor and troubleshoot management cluster deployments, review: The log file listed in the terminal output Logs of the command\n    execution can also be found at…\n</p>\n\n<p>The log from your cloud provider module for Cluster API. Retrieve the most recent one as follows:</p>\n\n<ul>\n    <li>\n        Search your tanzu management-cluster create output for Bootstrapper created. Kubeconfig: and copy the kubeconfig file path listed.\n        The file is in ~/.kube-tkg/tmp/.\n    </li>\n\n    <li>\n        Run the following, based on your cloud provider:\n\n        <ul>\n            <li>\n                vSphere:\n                <code\n                    >kubectl logs deployment.apps/capv-controller-manager -n capv-system manager --kubeconfig &lt;PATH-TO-KUBECONFIG&gt;\n                </code>\n            </li>\n\n            <li>\n                Amazon Web Services (AWS):\n                <code>\n                    kubectl logs deployment.apps/capa-controller-manager -n capa-system manager --kubeconfig &lt;PATH-TO-KUBECONFIG&gt;\n                </code>\n            </li>\n\n            <li>\n                Azure:\n                <code>\n                    kubectl logs deployment.apps/capz-controller-manager -n capz-system manager --kubeconfig\n                    &lt;PATH-TO-KUBECONFIG&gt;</code\n                >\n            </li>\n        </ul>\n    </li>\n</ul>\n"
        },
        {
            "topicTitle": "Cleanup after an unsuccessful management deployment",
            "topicIds": ["ctx-management-clusters"],
            "htmlContent": "\n\n<p>\n    When a management cluster fails to deploy successfully (or partially deploys), it may leave orphaned objects in your bootstrap\n    environment.\n</p>\n\n<p>Clean the bootstrap environment prior to a subsequent attempt of redeploying the management cluster.</p>\n\n<p>If the management cluster got partially created, attempt to delete the resources for the failed cluster:</p>\n\n<p><code>tanzu management-cluster delete &lt;YOUR-CLUSTER-NAME&gt; </code></p>\n\n<p>Next, if the bootstrap cluster still exists, delete it:</p>\n\n<p>\n    <code> kind get clusters tkg-kind-b4o9sn5948199qbgca8d</code>\n</p>\n\n<code> kind delete cluster --name tkg-kind-b4o9sn5948199qbgca8d</code>\n\n<p>Use docker to stop and remove any running containers related to the bootstrap process</p>\n"
        },
        {
            "topicTitle": "Connect to Cluster Nodes with SSH",
            "topicIds": ["ctx-workload-clusters"],
            "htmlContent": "\n\n<p>\n    You can use SSH to connect to nodes in management clusters and workload clusters. The SSH key pair that you created when you deployed\n    the management cluster must be available on the machine on which you run the SSH command.\n</p>\n\n<p>The SSH key used to access clusters are associated with the following Linux users:</p>\n\n<ul>\n    <li>vSphere management cluster and workload cluster nodes running on both Photon OS and Ubuntu: capv</li>\n    <li>Amazon EC2 bastion nodes: ubuntu</li>\n    <li>Amazon EC2 management cluster and workload cluster nodes running on Ubuntu: ubuntu</li>\n    <li>Amazon EC2 management cluster and workload cluster nodes running on Amazon Linux: ec2-user</li>\n\n    <li>Azure management cluster and workload cluster nodes (always Ubuntu): capi</li>\n</ul>\n\n<p>To connect to a node by using SSH, run one of the following commands from a machine containing the SSH key:</p>\n\n<ul>\n    <li>vSphere nodes: <code>ssh capv@&lt;em&gt;node_address</code></li>\n    <li>Amazon EC2 bastion nodes, management cluster, and workload nodes on Ubuntu:<code>ssh ubuntu@node_address</code></li>\n    <li>Amazon EC2 management cluster and workload nodes running on Amazon Linux: <code>ssh ec2-user@node_address</code></li>\n    <li>Azure nodes: <code>ssh capi@&lt;em&gt;node_address</code></li>\n</ul>\n\n<p>\n    Each cluster host contains the public key. The hosts will not accept an SSH password <code>(PasswordAuthentication is set to no).</code>\n</p>\n"
        },
        {
            "topicTitle": "Docker Daemon prerequisite",
            "topicIds": ["ctx-welcome", "ctx-getting-started", "ctx-management-clusters", "ctx-unmanaged-clusters"],
            "htmlContent": "\n<p>\n    Ensure your Docker engine is installed and has adequate resources. The minimum requirements with no other containers running are: 6 GB\n    of RAM and 4 CPUs.\n</p>\n<p>\n    Linux: Run docker system info <br />\n    Mac: Select Settings > Resources > Advanced\n</p>\n\n<p>\n    Bootstrapping a cluster to Docker from a Windows bootstrap machine is currently experimental, for more information, see\n    <a style=\"color: white\" href=\"https://tanzucommunityedition.io/docs/v0.12/ref-windows-capd/\" target=\"_blank\"\n        >Docker-based Clusters on Windows.\n    </a>\n</p>\n\n<a\n    style=\"color: white\"\n    href=\"https://tanzucommunityedition.io/docs/v0.12/support-matrix/#support-matrix-summary-for-operating-system-and-infrastructure-provider\"\n    target=\"_blank\"\n>\n    View the Support Matrix for supported operating systems and infrastructure providers.\n</a>\n"
        },
        {
            "topicTitle": "Limitations with Unmanaged Clusters",
            "topicIds": ["ctx-unmanaged-clusters"],
            "htmlContent": "\n\n<h4>Can’t Upgrade Kubernetes</h4>\n<p>\n    By design, unmanaged clusters do not lifecycle-manage Kubernetes. They are not meant to be long-running with real workloads. To change\n    Kubernetes versions, delete the existing cluster and create a new cluster with a different configuration.\n</p>\n<h4>Deploy to Windows</h4>\n<p>\n    <code>kind</code>, the default provider, has several known limitations when deploying to Windows. For example,\n    <u>deploying a load balancer has networking considerations.</u> Be sure to familiarize yourself with the <code>kind</code>\n    <u>documentation</u> in order to <u>customize your unmanaged-cluster deployment</u> for your needs.\n</p>\n"
        },
        {
            "topicTitle": "Managed Clusters",
            "topicIds": ["ctx-welcome", "ctx-getting-started", "ctx-workload-clusters"],
            "htmlContent": "\n\n<p>\n    Managed clusters are deployed and managed by a Tanzu management cluster. This is the primary deployment model for clusters in the Tanzu\n    ecosystem and is recommended for production scenarios. To bootstrap managed clusters, you first need a management cluster. When creating\n    a management cluster, a bootstrap cluster is created locally and is used to then create the management cluster.\n</p>\n\n<p>\n    Once the management cluster has been created, the bootstrap cluster will perform a move (aka pivot) of all management objects to the\n    management cluster. From this point forward, the management cluster is responsible for managing itself and any new clusters you create.\n    These new clusters, managed by the management cluster, are referred to as workload clusters.\n</p>\n"
        },
        {
            "topicTitle": "Management Clusters",
            "topicIds": ["ctx-getting-started", "ctx-management-clusters"],
            "htmlContent": "\n\n<p>\n    The management cluster provides management and operations for Tanzu. It runs Cluster-API which is used to manage workload clusters and\n    multi-cluster services. The workload cluster(s) are where developer’s workloads run.\n</p>\n\n<p>\n    When you create a management cluster, a bootstrap cluster is created on your local machine. This is a\n    <a style=\"color: white\" href=\"https://kind.sigs.k8s.io/\" target=\"_blank\"> Kind</a> based cluster, which runs via Docker. The bootstrap\n    cluster creates a management cluster on your specified provider. The information for how to manage clusters in the target environment is\n    then pivoted into the management cluster. At this point, the local bootstrap cluster is deleted.\n</p>\n"
        },
        {
            "topicTitle": "Persistent Volumes with Storage Classes",
            "topicIds": ["ctx-workload-clusters"],
            "htmlContent": "\n\n<h4>Supported Storage Types</h4>\n\n<p>\n    Tanzu Community Edition supports StorageClass objects for different storage types, provisioned by Kubernetes internal (“in-tree”) or\n    external (“out-of-tree”) plug-ins.\n</p>\n\n<h4>Storage Types</h4>\n\n<ul>\n    <li>vSphere Cloud Native Storage (CNS)</li>\n    <li>Amazon EBS</li>\n\n    <li>Azure Disk</li>\n\n    <li>iSCSI</li>\n\n    <li>NFS</li>\n</ul>\n\n<h4>Plug-in Locations</h4>\n\n<ul>\n    <li>\n        Kubernetes internal (“in-tree”) storage. - Ships with core Kubernetes; provider values are prefixed with kubernetes.io, e.g.\n        kubernetes.io/aws-ebs.\n    </li>\n\n    <li>\n        External (“out-of-tree”) storage. - Can be anywhere defined by provider value, e.g. csi.vsphere.vmware.com. - Follow the Container\n        Storage Interface (CSI) standard for external storage.\n    </li>\n</ul>\n\n<h4>Default Storage Classes</h4>\n\n<p>\n    Tanzu provides default StorageClass objects that let workload cluster users provision persistent storage on their infrastructure in a\n    turnkey environment, without needing StorageClass objects created by a cluster administrator.\n</p>\n\n<p>\n    The ENABLE_DEFAULT_STORAGE_CLASS variable is set to true by default in the cluster configuration file passed to --file option of tanzu\n    cluster create, to enable the default storage class for a workload cluster.\n</p>\n\n<ul>\n    <li>\n        <a style=\"color: white\" href=\"https://tanzucommunityedition.io/docs/v0.12/storage/#default-storage-classes\" target=\"_blank\">\n            View Tanzu default Storage Class definitions</a\n        >\n    </li>\n    <li>\n        <a style=\"color: white\" href=\"https://tanzucommunityedition.io/docs/v0.12/storage/#create-a-custom-storage-class\" target=\"_blank\">\n            Create a Custom Storage Class</a\n        >\n    </li>\n</ul>\n"
        },
        {
            "topicTitle": "Scale Management Cluster",
            "topicIds": ["ctx-management-clusters"],
            "htmlContent": "\n\n<p>\n    After you deploy a management cluster, you can scale it up or down by increasing or reducing the number of node VMs that it contains. To\n    scale a management cluster, use the tanzu cluster scale command with one or both of the following options:\n</p>\n\n<ul>\n    <li><code>--controlplane-machine-count</code> changes the number of management cluster control plane nodes.</li>\n\n    <li><code>--worker-machine-count</code> changes the number of management cluster worker nodes.</li>\n</ul>\n\n<p>\n    Because management clusters run in the tkg-system namespace rather than the default namespace, you must also specify the --namespace\n    option when you scale a management cluster.\n</p>\n\n<ol>\n    <li>\n        Run tanzu login before you run tanzu cluster scale to check that the management cluster you wish to scale is the current context of\n        the Tanzu CLI.\n    </li>\n\n    <li>\n        To scale a production management cluster that you originally deployed with 3 control plane nodes and 5 worker nodes to 5 and 10\n        nodes respectively, run the following command:\n        <code>\n            tanzu cluster scale &lt;MANAGEMENT-CLUSTER-NAME&gt; --controlplane-machine-count 5 --worker-machine-count 10 --namespace\n            tkg-system\n        </code>\n    </li>\n</ol>\n\n<p>\n    If you initially deployed a development management cluster with one control plane node and you scale it up to 3 control plane nodes,\n    Tanzu Community Edition automatically enables stacked HA on the control plane.\n</p>\n\n<p><b> IMPORTANT: </b> Do not change context or edit the .kube-tkg/config file while Tanzu Community Edition operations are running.</p>\n"
        },
        {
            "topicTitle": "Scale Workload Clusters",
            "topicIds": ["ctx-workload-clusters"],
            "htmlContent": "\n\n<p>\n    Cluster Autoscaler is a Kubernetes program that automatically scales Kubernetes clusters depending on the demands on the workload\n    clusters. For more information about Cluster Autoscaler, see the following documentation in GitHub:\n</p>\n\n<ul>\n    <li><u> Cluster Autoscaler </u> introduction and documentation.</li>\n    <li><u> Frequently Asked Questions</u> about Cluster Autoscaler and how it relates to alternative autoscaling approaches.</li>\n    <li><u>Cluster Autoscaler on Cluster API</u>for information about cluster-api provider implementation for cluster autoscaler.</li>\n</ul>\n\n<p>\n    By default, Cluster Autoscaler is disabled. To enable Cluster Autoscaler in a workload cluster, set the ENABLE_AUTOSCALER to true and\n    set the AUTOSCALER_ options in the workload cluster configuration file or as environment variables before running tanzu cluster create\n    --file. Sample workload cluster template files are available here: <u>AWS, Azure, vSphere.</u>\n</p>\n\n<a style=\"color: white\" href=\"https://tanzucommunityedition.io/docs/v0.12/scale-cluster/\" target=\"_blank\">\n    View more on scaling Workload Clusters\n</a>\n"
        },
        {
            "topicTitle": "Secure automated Ingress on AWS",
            "topicIds": ["ctx-workload-clusters"],
            "htmlContent": "\n\n<p>\n    Now that you have provisioned workload clusters, it’s time to use it. If you want to deploy applications on this cluster you will need\n    some packages installed on it that will give you some capabilities to improve the usage experience of the cluster.\n</p>\n\n<p>Install and properly configure the following Kubernetes services:</p>\n\n<ul>\n    <li>\n        <b>Ingress controller.</b> This is the entry point to your cluster. Requests will most likely be routed to applications running in\n        Kubernetes via an Ingress Controller.\n    </li>\n\n    <li>\n        <b>Certificate management.</b>\n        Because your services should be exposed using a secure transport (HTTPS), you need to generate TLS certificates that your ingress\n        controller will use to secure your application’s communication with the public.\n    </li>\n\n    <li>\n        <b>DNS management.</b>You want your applications to listen on a specific domain name, but you don’t know (or want) to learn how to\n        configure Route53 or your DNS provider. With automatic DNS management integration, your services will directly be exposed where you\n        expect them.\n    </li>\n</ul>\n\n<p>With these services deployed, it becomes easier to deploy an application on a cluster and access it through a named URL.</p>\n\n<a style=\"color: white\" href=\"https://tanzucommunityedition.io/docs/v0.12/solutions-secure-ingress/\" target=\"_blank\">\n    View the full guide of Secure automated Ingress on AWS\n</a>\n"
        },
        {
            "topicTitle": "Supported Infrastructure providers",
            "topicIds": ["ctx-welcome", "ctx-getting-started", "ctx-management-clusters"],
            "htmlContent": "\n\n<p>The following infrastructure providers are supported: Amazone Web Services, Docker, Microsoft Azure, VMware vSphere.</p>\n\n<p>Tanzu Community Edition supports the following Kubernetes versions: 1.21.2, 1.20.8, 1.19.12</p>\n\n<a\n    style=\"color: white\"\n    href=\"https://tanzucommunityedition.io/docs/v0.12/support-matrix/#support-matrix-summary-for-operating-system-and-infrastructure-provider\"\n    target=\"_blank\"\n>\n    View the Support Matrix for supported operating systems and infrastructure providers.\n</a>\n"
        },
        {
            "topicTitle": "Tanzu Packages",
            "topicIds": ["ctx-welcome", "ctx-workload-clusters", "ctx-unmanaged-clusters"],
            "htmlContent": "\n\n<p>\n    Packages extend the functionality of Tanzu Community Edition. You can discover and deploy packages through the Tanzu CLI. A Tanzu\n    package is an aggregation of Kubernetes configurations, and its associated software container image, into a versioned and distributable\n    bundle, that can be deployed as an OCI container image. Packages are installed into a Tanzu cluster.\n</p>\n\n<p>Run tanzu package list in your terminal to view avaliable packages on an unmanaged cluster, or workload cluster.</p>\n\n<a style=\"color: white\" href=\"https://tanzucommunityedition.io/docs/main/architecture/#tanzu-clusters\" target=\"_blank\"\n    >Learn more about managing Tanzu Packages.</a\n>\n"
        },
        {
            "topicTitle": "Troubleshoot Clusters with Tanzu Diagnostics",
            "topicIds": ["ctx-management-clusters"],
            "htmlContent": "\n\n<p>\n    Tanzu Community Edition comes with a diagnostics CLI plugin that helps with the task of collecting diagnostics data when debugging\n    installation issues. The plugin (based on the Crash-Diagnostics project) can automatically collect diagnostics data from either the\n    bootstrap, management, unmanaged, or a workload cluster (or all four) using the tanzu diagnostics collect command. Learn more about\n    Tanzu Diagnostics\n</p>\n\n<p>\n    When debugging a failed management cluster deployment, the kind bootstrap cluster is the key to being able to introspect and understand\n    what is happening. Any issues or errors that occur in the kind bootstrap cluster during bootstrapping will provide information about\n    potential problems in the final management cluster on the target provider. If the bootstrap process is stuck or did not finish\n    successfully, use the diagnostics plugin to collect logs and cluster information.\n</p>\n\n<a style=\"color: white\" href=\"https://tanzucommunityedition.io/docs/v0.12/tanzu-diagnostics/\" target=\"_blank\">\n    Learn more about Tanzu Diagnostics</a\n>\n"
        },
        {
            "topicTitle": "Troubleshooting cluster creation",
            "topicIds": ["ctx-getting-started", "ctx-management-clusters"],
            "htmlContent": "\n\n<p>If your bootstrap cluster fails to pivot to a management cluster by hanging or returning an error similar to the following:</p>\n\n<code\n    >Error: unable to set up management cluster: unable to wait for cluster and get the cluster kubeconfig: error waiting for cluster to be\n    provisioned (this may take a few minutes): cluster creation failed, reason:'NatGatewaysReconciliationFailed', message:'3 of 8 completed'\n</code>\n\n<p>Access the pod log for your respective provider to identify the problem.</p>\n\n<p>\n    ## For AWS <br />\n    <code>\n        docker exec &lt;CONTAINER_ID&gt; kubectl logs --namespace capa-system --selector\n        cluster.x-k8s.io/provider=infrastructure-aws,control-plane=controller-manager -c manager --kubeconfig /etc/kubernetes/admin.conf\n    </code>\n</p>\n<p>\n    ## For Azure <br />\n    <code>\n        docker exec &lt;CONTAINER_ID&gt; kubectl logs --namespace capz-system --selector\n        cluster.x-k8s.io/provider=infrastructure-azure,control-plane=controller-manager -c manager --kubeconfig /etc/kubernetes/admin.conf\n    </code>\n</p>\n<p>\n    ## For Docker <br />\n    <code>\n        docker exec &lt;CONTAINER_ID&gt; kubectl logs --namespace capd-system --selector\n        cluster.x-k8s.io/provider=infrastructure-docker,control-plane=controller-manager -c manager --kubeconfig /etc/kubernetes/admin.conf\n    </code>\n</p>\n<p>\n    ## For vSphere <br />\n    <code>\n        docker exec &lt;CONTAINER_ID&gt; kubectl logs --namespace capv-system --selector\n        cluster.x-k8s.io/provider=infrastructure-vsphere,control-plane=controller-manager -c manager --kubeconfig /etc/kubernetes/admin.conf\n    </code>\n</p>\n"
        },
        {
            "topicTitle": "Unmanaged Clusters",
            "topicIds": ["ctx-welcome", "ctx-getting-started", "ctx-unmanaged-clusters"],
            "htmlContent": "\n\n<p>\n    An unmanaged cluster offers a single node, local workstation cluster suitable for a development/test environment. It requires minimal\n    local resources and is fast to deploy. It provides support for running multiple clusters. The default Tanzu Community Edition package\n    repository is automatically installed when you deploy an unmanaged cluster.\n</p>\n"
        },
        {
            "topicTitle": "Workload Clusters",
            "topicIds": ["ctx-getting-started", "ctx-workload-clusters"],
            "htmlContent": "\n\n<p>\n    After you deploy the management cluster, you can deploy a workload cluster. The workload cluster is deployed by the management cluster.\n    The workload cluster is used to run your application workloads. The workload cluster is deployed using the Tanzu CLI.\n</p>\n\n<p>A management cluster is required in order to view workload clusters.</p>\n"
        }
    ]
}
